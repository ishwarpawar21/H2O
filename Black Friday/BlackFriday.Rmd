---
title: "Black Friday Practice Problem"
output:
  html_document: default
  html_notebook: default
---
The data set has two parts: Train and Test. Train data set contains 550068 observations. Test data set contains 233599 observations.


```{r}
install.packages("data.table")
install.packages("h2o")
install.packages("dummies")
install.packages("gmodels")
```
```{r}
library(data.table)
```

Read the tranning and test data using fread

```{r}
train <- fread("train.csv", stringsAsFactors = T)
test <- fread("test.csv", stringsAsFactors = T)
```

Check the no. of rows and structure of the traning and test data set

```{r}
nrow(train)
nrow(test)

str(train)
str(test)
```


12 variables, 2 of which seems to have so many NAs.
Purchase is the dependent variable, rest 11 are independent variables.

Looking at the nature of Purchase variable (continuous), we can infer that this is a regression problem. Even though, the competition is closed but we can still check our score and evaluate how good we could have done. Let’s make our first submission.

With all the data points we’ve got, we can make our first set of prediction using mean. This is because, mean prediction will give us a good approximation of prediction error. Taking this as baseline prediction, our model won’t do worse than this.

```{r}
#predction using Mean

sub_mean <- data.frame(user_Id= test$User_ID, prodcut_Id = test$Product_ID, purchase = mean(train$Purchase))
write.csv(sub_mean, file = "first_sub.csv", row.names = F)
```



Summary of test and training
```{r}
summary(test)
summary(train)
```

Let’s combine the data set. I’ve used rbindlist function from data.table, since it’s faster than rbind.

```{r}
test[,Purchase := mean(train$Purchase)]
c <- list(train, test)
combin <- rbindlist(c)
```


STEP 2:
Data Exploration using data.table and ggplot2
Lets try to understand the relationship among given variables. Let’s start with univariate.


```{r}

#analyzing gender variable
combin[,prop.table(table(Gender))] 

#Age Variable
combin[,prop.table(table(Age))]

#City Category Variable
combin[,prop.table(table(City_Category))]

#Stay in Current Years Variable
combin[,prop.table(table(Stay_In_Current_City_Years))]

#unique values in ID variables
length(unique(combin$Product_ID))

length(unique(combin$User_ID))

#missing values
colSums(is.na(combin))

```

Inferences we can generate from univariate analysis:

- need to encode Gender variable into 0 and 1 and categorical 
- We’ll also need to re-code the Age bins.
- Since there are three levels in City_Category, we can do one-hot encoding.
- The “4+” level of Stay_in_Current_Years needs to be revalued.
- The data set does not contain all unique IDs. This gives us enough hint for feature engineering.
- Only 2 variables have missing values. In fact, a lot of missing values, which could be capturing a hidden trend. We’ll need to treat them differently.


```{r}
library(ggplot2)
```

```{r}
#Age vs Gender
ggplot(combin, aes(Age, fill = Gender)) + geom_bar()

#Age vs City_Category
ggplot(combin, aes(Age, fill = City_Category)) + geom_bar()

```


```{r}
library(gmodels)
CrossTable(combin$Occupation, combin$City_Category)
```


3.DATA MANUPULATON

In this step  we’ll create new variables, revalue existing variable and treat missing values. In simple words, we’ll get our data ready for modeling stage.

Let’s start with missing values. We saw Product_Category_2 and Product_Category_3 had a lot of missing values.
This suggests a hidden trend which can be mapped by creating a new variable. 
So, we’ll create a new variable which will capture NAs as 1 and non-NAs as 0 in the variables Product_Category_2 and Product_Category_3.

```{r}
#create a new variable for missing values
combin[,Product_Category_2_NA := ifelse(sapply(combin$Product_Category_2, is.na) ==    TRUE,1,0)]
combin[,Product_Category_3_NA := ifelse(sapply(combin$Product_Category_3, is.na) ==  TRUE,1,0)]
```

Let’s now impute the missing values with any arbitrary number. Let’s take -999

```{r}
#impute missing values
combin[,Product_Category_2 := ifelse(is.na(Product_Category_2) == TRUE, "-999",  Product_Category_2)]
combin[,Product_Category_3 := ifelse(is.na(Product_Category_3) == TRUE, "-999",  Product_Category_3)]
```

Feature engineering, lastly, we’ll  revalue variable levels as inferred from our univariate analysis.

```{r}
#set column level
levels(combin$Stay_In_Current_City_Years)[levels(combin$Stay_In_Current_City_Years) ==  "4+"] <- "4"




#recoding age groups
levels(combin$Age)[levels(combin$Age) == "0-17"] <- 0
levels(combin$Age)[levels(combin$Age) == "18-25"] <- 1
levels(combin$Age)[levels(combin$Age) == "26-35"] <- 2
levels(combin$Age)[levels(combin$Age) == "36-45"] <- 3
levels(combin$Age)[levels(combin$Age) == "46-50"] <- 4
levels(combin$Age)[levels(combin$Age) == "51-55"] <- 5
levels(combin$Age)[levels(combin$Age) == "55+"] <- 6

#convert age to numeric
combin$Age <- as.numeric(combin$Age)

#convert Gender into numeric
combin[, Gender := as.numeric(as.factor(Gender)) - 1]
```

During univariate analysis, we discovered that ID variables have lesser unique values as compared to total observations in the data set. It means there are User_IDs or Product_IDs must have appeared repeatedly in this data set.

Let’s create a new variable which captures the count of these ID variables. Higher user count suggests that a particular user has purchased products multiple times. High product count suggests that a product has been purchased many a times, which shows its popularity.


```{r}
#User Count
combin[, User_Count := .N, by = User_ID]

#Product Count
combin[, Product_Count := .N, by = Product_ID]
```

Also, we can calculate the mean purchase price of a product. Because, lower the purchase price, higher will be the chances of that product being bought or vice versa. Similarly, we can create another variable which maps the average purchase price by user i.e. how much purchase (on an average) is made by a user. Let’s do it.

```{r}
#Mean Purchase of Product
combin[, Mean_Purchase_Product := mean(Purchase), by = Product_ID]

#Mean Purchase of User
combin[, Mean_Purchase_User := mean(Purchase), by = User_ID]

library(dummies)
combin <- dummy.data.frame(combin, names = c("City_Category"), sep = "_")

```


```{r}
sapply(combin, class)
#converting Product Category 2 & 3
combin$Product_Category_2 <- as.integer(combin$Product_Category_2)
combin$Product_Category_3 <- as.integer(combin$Product_Category_3)
```


 

4. Model Building using H2O

Divide data into test and tarin
```{r}
main.train <- combin[1:nrow(train),]
main.test <- combin[-(1:nrow(train)),]
```


As discovered in beginning that the variable Product_Category_1 in train has some noise. Let’s remove it as well by selecting all rows in Product_Category_1 upto 18, thereby dropping rows which has category level 19 & 20.

```{r}
main.train <- main.train[main.train$Product_Category_1 <= 18,]
```


launch the H2O cluster, write
This commands tell H2O to use all the CPUs on the machine, which is recommended. For larger data sets (say > 1,000,000 rows), h2o  recommends running cluster on a server with high memory for optimal performance. Once the instance starts successfully, you can also check its status using
```{r}
library(h2o)
localH2O <- h2o.init(nthreads = -1)
h2o.init()
```


Let’s now transfer the data from R to h2o instance. It can be accomplished using as.h2o command.

```{r}
train.h2o <- as.h2o(main.train)
test.h2o <-as.h2o(main.test)
```


Using column index, we need to identify variables to be used in modeling as follows:

```{r}
colnames(train.h2o)
#dependent variable (Purchase)
y.dependent <- 14

#independent variables (dropping ID variables)
x.independent <- c(3:13,15:20)
nrow(train.h2o)
```


Multiple Regression in H2O
```{r}
regression.model <- h2o.glm( y = y.dependent, x = x.independent, training_frame = train.h2o, family = "gaussian")
```

```{r}
h2o.performance(regression.model)
```

Regression gives a poor R² value i.e. 0.326. It means that only 32.6% of the variance in the dependent variable is explained by independent variable and rest is unexplained. This shows that regression model is unable to capture non linear relationships.


Make predictions 

```{r}
predict.reg <- as.data.frame(h2o.predict(regression.model, test.h2o))
sub_reg <- data.frame(User_ID = test$User_ID, Product_ID = test$Product_ID, Purchase =  predict.reg$predict)
write.csv(sub_reg, file = "sub_reg.csv", row.names = F)
```


Random Forest in H2O
#Random Forest
```{r}
rforest.model <- h2o.randomForest(y=y.dependent, x=x.independent, training_frame = train.h2o, ntrees = 1000, mtries = 3, max_depth = 4, seed = 1122)
```



```{r}

h2o.performance(rforest.model)
#check variable importance
h2o.varimp(rforest.model)
```




#making predictions on unseen data
```{r}
system.time(predict.rforest <- as.data.frame(h2o.predict(rforest.model, test.h2o)))
```


GBM in H2O
If you are new to GBM, I’d suggest you to check the resources given in the start of this section. We can implement GBM in H2O using a simple line of code:

#GBM
```{r}

nrow(train.h2o)
gbm.model <- h2o.gbm(y=y.dependent, x=x.independent, training_frame = train.h2o, ntrees = 1000, max_depth = 4, learn_rate = 0.01, seed = 1122)

```


```{r}


h2o.performance (gbm.model)
#check variable importance
h2o.varimp(gbm.model)

h2o.rmsle(gbm.model)
predict.gbm <- as.data.frame(h2o.predict(gbm.model, test.h2o))

```




Deep Learning in H2O

```{r}

system.time(
             dlearning.model <- h2o.deeplearning(y = y.dependent,
             x = x.independent,
             training_frame = train.h2o,
             epoch = 60,
             hidden = c(100,100),
             activation = "Rectifier",
             seed = 1122
             )
)

```


```{r}
h2o.performance(dlearning.model)
```